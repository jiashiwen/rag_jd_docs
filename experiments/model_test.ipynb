{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 按token输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from vllm import LLM\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import sys\n",
    "\n",
    "\n",
    "def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "    sys.stdout.write(token)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "llm_llama3 = VLLM(\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "inputs = tokenizer([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextIteratorStreamer(tokenizer)\n",
    "\n",
    "# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "# generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n",
    "# thread = Thread(target=llm_llama3.generate, kwargs=generation_kwargs)\n",
    "# thread.start()\n",
    "# generated_text = \"\"\n",
    "# for new_text in streamer:\n",
    "#     generated_text += new_text\n",
    "#     print(generated_text)\n",
    "llm_llama3.generate([\"nihao\"], streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfomer streaming test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import half\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "llm_llama3 = LLM(\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    dtype=half,\n",
    "    # temperature=0.2,\n",
    "    # do_sample=True,\n",
    "    # repetition_penalty=1.1,\n",
    "    # return_full_text=False,\n",
    "    # max_new_tokens=900,\n",
    ")\n",
    "\n",
    "# inputs = tok([\"世界你好\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tok)\n",
    "\n",
    "\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "# _ = llm_llama3.generate(**inputs, streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vllm import SamplingParams\n",
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "# llm_llama3 = LLM(\n",
    "#     model=model_name,\n",
    "#     tokenizer=model_name,\n",
    "#     # temperature=0.2,\n",
    "#     # do_sample=True,\n",
    "#     # repetition_penalty=1.1,\n",
    "#     # return_full_text=False,\n",
    "#     # max_new_tokens=900,\n",
    "# )\n",
    "\n",
    "llm_llama3 = LLM(\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    ")\n",
    "sampling_params = SamplingParams(top_p=0.95,\n",
    "                                 temperature=0.2,\n",
    "                                 repetition_penalty=1.1,\n",
    "                                 max_tokens=900,\n",
    "                                 )\n",
    "# streamer = TextStreamer(tok)\n",
    "# sampling_params = SamplingParams(top_p=0.95,\n",
    "#                                  temperature=0.2,\n",
    "#                                  #  do_sample=True,\n",
    "#                                  repetition_penalty=1.1,\n",
    "#                                  max_tokens=900,\n",
    "\n",
    "#                                  #  return_full_text=False,\n",
    "#                                  #  max_new_tokens=900,\n",
    "#                                  )\n",
    "\n",
    "\n",
    "outputs = llm_llama3.generate(\n",
    "    \"其中 LLM类主要运行离线程序、SamplingParams负责设置调用模型当中的一些参数\", sampling_params=sampling_params)\n",
    "\n",
    "# llm_llama3(\"其中 LLM类主要运行离线程序、SamplingParams负责设置调用模型当中的一些参数\")\n",
    "\n",
    "# for output in outputs:\n",
    "#     prompt = output.prompt\n",
    "#     generated_text = output.outputs[0].text\n",
    "#     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, temperature=0.2,\n",
    "                                             do_sample=True,\n",
    "                                             repetition_penalty=1.1,)\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tok)\n",
    "\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "_ = model.generate(**inputs, streamer=streamer,\n",
    "                   max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(llm_llama3)\n",
    "\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextIteratorStreamer(tok)\n",
    "\n",
    "# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "generated_text = \"\"\n",
    "for new_text in streamer:\n",
    "    generated_text += new_text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据文档生成问题示例"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
