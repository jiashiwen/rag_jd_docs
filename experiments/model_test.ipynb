{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 按token输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 16:41:00,384\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-22 16:41:00 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/root/models/Qwen1.5-1.8B-Chat', speculative_config=None, tokenizer='/root/models/Qwen1.5-1.8B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/root/models/Qwen1.5-1.8B-Chat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-22 16:41:01 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-22 16:41:01 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-22 16:41:03 model_runner.py:175] Loading model weights took 3.4653 GB\n",
      "INFO 05-22 16:41:04 gpu_executor.py:114] # GPU blocks: 5356, # CPU blocks: 1365\n",
      "INFO 05-22 16:41:06 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-22 16:41:06 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-22 16:41:10 model_runner.py:1017] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from vllm import LLM\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import sys\n",
    "\n",
    "\n",
    "def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "    sys.stdout.write(token)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "llm_llama3 = VLLM(\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n1. 你认为人工智能在医疗领域的应用前景如何？\\n2. 你认为人工智能在医疗领域的应用有哪些挑战和限制？\\n3. 你认为人工智能在医疗领域的应用有哪些优势和潜力？\\n4. 你认为人工智能在医疗领域的应用有哪些局限性？\\n5. 你认为人工智能在医疗领域的应用有哪些未来发展趋势？\\n6. 你认为人工智能在医疗领域的应用有哪些伦理和法律问题？\\n7. 你认为人工智能在医疗领域的应用有哪些社会影响？\\n8. 你认为人工智能在医疗领域的应用有哪些经济影响？\\n9. 你认为人工智能在医疗领域的应用有哪些技术挑战？\\n10. 你认为人工智能在医疗领域的应用有哪些未来挑战？\\n\\n11. 你认为人工智能在医疗领域的应用有哪些具体案例？\\n12. 你认为人工智能在医疗领域的应用有哪些成功的经验和教训？\\n13. 你认为人工智能在医疗领域的应用有哪些未解决的问题和挑战？\\n14. 你认为人工智能在医疗领域的应用有哪些未来研究方向？\\n15. 你认为人工智能在医疗领域的应用有哪些可能的伦理和法律问题？\\n16. 你认为人工智能在医疗领域的应用有哪些社会影响？\\n17. 你认为人工智能在医疗领域的应用有哪些经济影响？\\n18. 你认为人工智能在医疗领域的应用有哪些技术挑战？\\n19. 你认为人工智能在医疗领域的应用有哪些未来挑战？\\n20. 你认为人工智能在医疗领域的应用有哪些可能的应用场景？')]], llm_output=None, run=[RunInfo(run_id=UUID('a285b50a-6ec9-4d7c-b149-45592bfd19c0'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from transformers import TextIteratorStreamer\n",
    "inputs = tokenizer([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextIteratorStreamer(tokenizer)\n",
    "\n",
    "# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "# generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n",
    "# thread = Thread(target=llm_llama3.generate, kwargs=generation_kwargs)\n",
    "# thread.start()\n",
    "# generated_text = \"\"\n",
    "# for new_text in streamer:\n",
    "#     generated_text += new_text\n",
    "#     print(generated_text)\n",
    "llm_llama3.generate([\"nihao\"], streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfomer streaming test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 10:59:18 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='/root/models/Qwen1.5-1.8B-Chat', speculative_config=None, tokenizer='/root/models/Qwen1.5-1.8B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/root/models/Qwen1.5-1.8B-Chat)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 10:59:19 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-23 10:59:20 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 05-23 10:59:21 model_runner.py:175] Loading model weights took 3.4653 GB\n",
      "INFO 05-23 10:59:22 gpu_executor.py:114] # GPU blocks: 5356, # CPU blocks: 1365\n",
      "INFO 05-23 10:59:24 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-23 10:59:24 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-23 10:59:28 model_runner.py:1017] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "llm_llama3 = LLM(\n",
    "    model=model_name,\n",
    "    tokenizer=model_name,\n",
    "    # temperature=0.2,\n",
    "    # do_sample=True,\n",
    "    # repetition_penalty=1.1,\n",
    "    # return_full_text=False,\n",
    "    # max_new_tokens=900,\n",
    ")\n",
    "\n",
    "# inputs = tok([\"世界你好\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tok)\n",
    "\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "# _ = llm_llama3.generate(**inputs, streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '其中 LLM类主要运行离线程序、SamplingParams负责设置调用模型当中的一些参数', Generated text: '，如batch_size、num_epochs等，以及在训练过程中监控模型的性能指标，如loss、accuracy等。例如：\\n\\n```python\\nclass SamplingParams:\\n    def __init__(self, batch_size=32, num_epochs=10):\\n        self.batch_size = batch_size\\n        self.num_epochs = num_epochs\\n\\n    def sample(self, inputs, targets):\\n        # 这里是用于生成样本数据的代码，根据输入和目标进行随机抽样\\n        pass\\n```\\n\\n在这个例子中，`sample`方法接受两个参数：`inputs`和`targets`，分别表示输入和目标数据。它会根据这些数据生成一个样本数据集，并返回这个样本数据集。\\n\\n需要注意的是，LSTM类中的`sample`方法需要依赖于具体的实现细节，比如使用的数据结构（如numpy数组或张量）、随机数生成算法（如numpy.random.choice或sklearn.choice）以及具体的训练策略（如梯'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from vllm import SamplingParams\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tok)\n",
    "sampling_params = SamplingParams(top_p=0.95,\n",
    "                                 temperature=0.2,\n",
    "                                 #  do_sample=True,\n",
    "                                 repetition_penalty=1.1,\n",
    "                                 max_tokens=900,\n",
    "\n",
    "                                 #  return_full_text=False,\n",
    "                                 #  max_new_tokens=900,\n",
    "                                 )\n",
    "\n",
    "\n",
    "outputs = llm_llama3.generate(\n",
    "    \"其中 LLM类主要运行离线程序、SamplingParams负责设置调用模型当中的一些参数\", sampling_params=sampling_params)\n",
    "\n",
    "# llm_llama3(\"其中 LLM类主要运行离线程序、SamplingParams负责设置调用模型当中的一些参数\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, temperature=0.2,\n",
    "                                             do_sample=True,\n",
    "                                             repetition_penalty=1.1,)\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tok)\n",
    "\n",
    "# Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "_ = model.generate(**inputs, streamer=streamer,\n",
    "                   max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(llm_llama3)\n",
    "\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextIteratorStreamer(tok)\n",
    "\n",
    "# Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "generated_text = \"\"\n",
    "for new_text in streamer:\n",
    "    generated_text += new_text\n",
    "    print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
