{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以京东云官网文档为基础的RAG实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "\n",
    "%pip install langchain_community\n",
    "%pip install langchain\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "%pip install vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "### 为文档添加.txt后缀，便于后期文档处理\n",
    "https://github.com/jiashiwen/datatoolkits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    './jdcloud-docs', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量存入 clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community.vectorstores.clickhouse as clickhouse\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "settings = clickhouse.ClickhouseSettings(\n",
    "    table=\"jd_docs\", username=\"default\", password=\"root\", host=\"10.0.16.88\")\n",
    "docsearch = clickhouse.Clickhouse.from_documents(\n",
    "    chunked_docs, embeddings, config=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 m3e 进行向量检索\n",
    "import langchain_community.vectorstores.clickhouse as clickhouse\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"moka-ai/m3e-large\", model_kwargs=model_kwargs)\n",
    "\n",
    "\n",
    "settings = clickhouse.ClickhouseSettings(\n",
    "    table=\"jd_docs_m3e\", username=\"default\", password=\"root\", host=\"10.0.16.88\")\n",
    "docsearch = clickhouse.Clickhouse.from_documents(\n",
    "    chunked_docs, embeddings, config=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从 clickhouse 创建 文档检索 retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import langchain_community.vectorstores.clickhouse as clickhouse\n",
    "# import os\n",
    "# os.environ[\"http_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "# os.environ[\"https_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "# 使用 m3e-large embemdding\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"/root/models/moka-ai-m3e-large\", model_kwargs=model_kwargs)\n",
    "\n",
    "settings = clickhouse.ClickhouseSettings(\n",
    "    table=\"jd_docs_m3e\", username=\"default\", password=\"Git785230\", host=\"10.0.1.94\")\n",
    "ck_db = clickhouse.Clickhouse(embeddings, config=settings)\n",
    "\n",
    "# retriever = ck_db.as_retriever(\n",
    "#     search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "retriever = ck_db.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 1, 'score_threshold': 0.8})\n",
    "\n",
    "\n",
    "# r = retriever.invoke(\"如何创建负载均衡\")\n",
    "r = retriever.invoke(\"京东云有和dataworks对标的产品吗\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo 选择好的中文模型\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, token=\"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\", quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试 llama3 中文 vllm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "# from vllm import LLM\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "\n",
    "model_name = \"/root/models/Llama3-Chinese-8B-Instruct\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, token=\"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\", quantization_config=bnb_config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, token=\"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\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, token=\"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\", quantization_config=bnb_config)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name, token=\"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\",)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置 LLM 链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo 了解langchain 语法流程，进行改造\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# text_generation_pipeline = pipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     task=\"text-generation\",\n",
    "#     temperature=0.2,\n",
    "#     do_sample=True,\n",
    "#     repetition_penalty=1.1,\n",
    "#     return_full_text=False,\n",
    "#     max_new_tokens=400,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "你是一个云技术专家\n",
    "使用以下检索到的Context回答问题。\n",
    "如果不知道答案，就说不知道。\n",
    "用中文回答问题。\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    "    # partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "p = prompt.format(context=\"huaidan\", question=\"你是谁\")\n",
    "p\n",
    "\n",
    "# llm(p)\n",
    "# llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用 rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = ck_db.as_retriever()\n",
    "rag_chain = {\"context\": retriever,\n",
    "             \"question\": RunnablePassthrough()} | llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = retriever.invoke(\"如何挂载弹性公网ip\")\n",
    "context\n",
    "llm_chain.invoke({\"context\": context, \"question\": \"如何挂载弹性公网ip\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"如何创建云主机\"\n",
    "\n",
    "# llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "r = rag_chain.invoke(question)\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
