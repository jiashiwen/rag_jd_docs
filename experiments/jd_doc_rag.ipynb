{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以京东云官网文档为基础的RAG实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "\n",
    "%pip install langchain_community\n",
    "%pip install langchain\n",
    "%pip install bitsandbytes\n",
    "%pip install accelerate\n",
    "%pip install vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "### 为文档添加.txt后缀，便于后期文档处理\n",
    "https://github.com/jiashiwen/datatoolkits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    './jdcloud-docs', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量存入 clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_community.vectorstores.clickhouse as clickhouse\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "settings = clickhouse.ClickhouseSettings(\n",
    "    table=\"jd_docs\", username=\"default\", password=\"root\", host=\"10.0.16.88\")\n",
    "docsearch = clickhouse.Clickhouse.from_documents(\n",
    "    chunked_docs, embeddings, config=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 m3e 进行向量检索\n",
    "import langchain_community.vectorstores.clickhouse as clickhouse\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"moka-ai/m3e-large\", model_kwargs=model_kwargs)\n",
    "\n",
    "\n",
    "settings = clickhouse.ClickhouseSettings(\n",
    "    table=\"jd_docs_m3e\", username=\"default\", password=\"root\", host=\"10.0.16.88\")\n",
    "docsearch = clickhouse.Clickhouse.from_documents(\n",
    "    chunked_docs, embeddings, config=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从 clickhouse 创建 文档检索 retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /root/models/moka-ai-m3e-large. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='创建实例\\n----\\n\\n\\n点击 进入控制台 按钮，进入后默认菜单为 实例列表页，可以通过控制台快速创建图数据库/图计算实例，注意：创建实例需要您的京东云账户中至少有50元钱，但创建及使用过程不会进行扣费。\\n\\n\\n#### 1、操作入口\\n\\n\\n* 进入图数据库与图计算控制台--实例列表页，可看到当前区域下各个实例的类型、引擎类型、规格及创建时间等。\\n* 图数据库用于对关系数据进行存储及查询服务，图计算用于做关联关系的分析和计算。\\n* 根据需求点击 创建图数据库 或 创建图计算 按钮，进入实例创建页面\\n\\n\\n[![\\\\\"1645340822451\\\\\"](\\\\\"https://jdcloud-portal.oss.cn-north-1.jcloudcs.com/cn/image/Elastic-Compute/Graph-Compute/1645340822451.png\\\\\")](\\\\\"https://jdcloud-portal.oss.cn-north-1.jcloudcs.com/cn/image/Elastic-Compute/Graph-Compute/1645340822451.png\\\\\")\\n\\n\\n#### 2、进行实例参数配置-图数据库\\n\\n\\n选择或输入实例的相关配置信息，实例配置的参数说明如下：\\n\\n\\n\\n\\n| 参数 | 说明 |\\n| --- | --- |\\n| 数据库类型 | 默认NebulaGraph |\\n| 版本 | 默认2.6.1 |\\n| 实例类型 | 默认集群 |\\n| 节点数量 | 用户根据需求可选1，3，5，7 |\\n| 存储类型 | 不同的存储类对应的实例规格的最大IOPS不同，当前支持本地SSD、SSD云盘，具体以控制台为准。 |\\n| 机器规格 | 根据需求情况选择不同机器规格，包括4核和8核两种可选项。 |\\n| 存储容量 | 根据数据量选择存储容量 |\\n| 地域 | 选择实例所在的地域， **不同地域资源的内网不互通，创建后不能更改** 。•建议选择最靠近您的地域，可降低访问时延、提高下载速度。 |\\n| 可用区 | 选择B或C区 |\\n| 私有网络 | 用户可选网络和子网 |\\n| 实例名称 | • 2-32位字符• 支持数字、小写字母、中文以及英文下划线 |\\n\\n\\n#### 3、进行实例参数配置-图计算\\n\\n\\n\\n\\n| 参数 | 说明 |\\n| --- | --- |\\n| 数计算引擎类型 | 默认JoyGraph |\\n| 版本 | 默认1.0.0 |\\n| 实例类型 | 仅支持单机 |\\n| 节点数 | 默认为1 |\\n| 存储类型 | 不同的存储类对应的实例规格的最大IOPS不同，当前支持本地SSD、SSD云盘，具体以控制台为准。 |\\n| 机器规格 | 根据需求情况选择不同机器规格，包括4核和8核两种可选项。 |\\n| 存储容量 | 根据数据量选择存储容量 |\\n| 地域 | 选择实例所在的地域， **不同地域资源的内网不互通，创建后不能更改** 。•关于地域的详细说明，请参考 [核心概念](\\\\\"https://docs.jdcloud.com/cn/rds/core-concepts\\\\\")。•建议选择最靠近您的地域，可降低访问时延、提高下载速度。 |\\n| 可用区 | 选择B或C区 |\\n| 私有网络 | 用户可选网络和子网 |\\n| 实例名称 | • 2-32位字符• 支持数字、小写字母、中文以及英文下划线 |\\n| 标签 | 填写对应的业务标签 |\\n\\n\\n#### 4、确认购买\\n\\n\\n信息输入完成后，点击 立即购买\\n\\n\\n#### 5、实例创建完成\\n\\n\\n确认购买后，进入实例列表页面，能看到创建的实例。\\n\\n\\n', metadata={'source': '/root/jd_docs/graph/create-instance.md'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import langchain_community.vectorstores.clickhouse as clickhouse\n",
    "# import os\n",
    "# os.environ[\"http_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "# os.environ[\"https_proxy\"] = \"http://127.0.0.1:1083\"\n",
    "# 使用 m3e-large embemdding\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"/root/models/moka-ai-m3e-large\", model_kwargs=model_kwargs)\n",
    "\n",
    "settings = clickhouse.ClickhouseSettings(\n",
    "    table=\"jd_docs_m3e\", username=\"default\", password=\"Git785230\", host=\"10.0.1.94\")\n",
    "ck_db = clickhouse.Clickhouse(embeddings, config=settings)\n",
    "\n",
    "# retriever = ck_db.as_retriever(\n",
    "#     search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "retriever = ck_db.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 1, 'score_threshold': 0.8})\n",
    "\n",
    "\n",
    "# r = retriever.invoke(\"如何创建负载均衡\")\n",
    "r = retriever.invoke(\"京东云有和dataworks对标的产品吗\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo 选择好的中文模型\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, token=\"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\", quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试 llama3 中文 vllm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "# from vllm import LLM\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "\n",
    "model_name = \"/root/models/Llama3-Chinese-8B-Instruct\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, token=\"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\", quantization_config=bnb_config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, token=\"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\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尝试qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain_community.llms import VLLM\n",
    "\n",
    "model_name = \"/root/models/Qwen1.5-1.8B-Chat\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name, token=\"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\", quantization_config=bnb_config)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name, token=\"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\",)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = VLLM(\n",
    "    model=model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置 LLM 链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo 了解langchain 语法流程，进行改造\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# text_generation_pipeline = pipeline(\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     task=\"text-generation\",\n",
    "#     temperature=0.2,\n",
    "#     do_sample=True,\n",
    "#     repetition_penalty=1.1,\n",
    "#     return_full_text=False,\n",
    "#     max_new_tokens=400,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "你是一个云技术专家\n",
    "使用以下检索到的Context回答问题。\n",
    "如果不知道答案，就说不知道。\n",
    "用中文回答问题。\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    "    # partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "p = prompt.format(context=\"huaidan\", question=\"你是谁\")\n",
    "p\n",
    "\n",
    "# llm(p)\n",
    "# llm_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用 rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = ck_db.as_retriever()\n",
    "rag_chain = {\"context\": retriever,\n",
    "             \"question\": RunnablePassthrough()} | llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = retriever.invoke(\"如何挂载弹性公网ip\")\n",
    "context\n",
    "llm_chain.invoke({\"context\": context, \"question\": \"如何挂载弹性公网ip\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"如何创建云主机\"\n",
    "\n",
    "# llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
    "r = rag_chain.invoke(question)\n",
    "print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
